# -*- coding: utf-8 -*-
"""HeartAttackPredict.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S8sU1_AdmbCKCr3pph5owWpX6Lf9FJAx
"""

import pandas as pd  # For data manipulation
from scipy.stats import zscore  # For z-score calculation

# Load the dataset
df = pd.read_csv('/content/Heartattack.csv')

# Initial data shape and missing values report
print("Initial data shape:", df.shape)
print("Missing values per column:\n", df.isnull().sum())

# Handle missing values by filling with mean
df.fillna(df.mean(), inplace=True)
print("Data shape after handling missing values:", df.shape)
print("Missing values per column after handling:\n", df.isnull().sum())

# Display dataset information
print("\nDataset information:")
print(df.info())

# Descriptive statistics of the dataset
print("\nDescriptive statistics:")
print(df.describe())

# Correlation matrix
print("\nCorrelation matrix:")
print(df.corr())

import numpy as np

# Z-score method for outlier detection
def detect_outliers_zscore(data, threshold=3):
    z_scores = np.abs((data - data.mean()) / data.std())
    outliers = (z_scores > threshold).sum()
    return outliers

# Detect and report outliers in each numeric column
numeric_columns = df.select_dtypes(include=[np.number]).columns
for col in numeric_columns:
    outliers = detect_outliers_zscore(df[col])
    print(f"Outliers in {col}: {outliers}")

# Remove outliers
df = df[(np.abs((df[numeric_columns] - df[numeric_columns].mean()) / df[numeric_columns].std()) <= 3).all(axis=1)]
print("Data shape after removing outliers:", df.shape)

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler


# Check the columns in the dataset
print("Columns in the dataset:", df.columns)

# Check the first few rows to ensure the column names and data look correct
print(df.head())

# Separate the target column
target_column = 'output'
output = df[target_column]
df = df.drop(columns=[target_column])

# Identify categorical and numeric columns
categorical_columns = df.select_dtypes(exclude=[np.number]).columns
numeric_columns = df.select_dtypes(include=[np.number]).columns

# Normalize numeric columns
scaler = StandardScaler()
df_normalized = pd.DataFrame(scaler.fit_transform(df[numeric_columns]), columns=numeric_columns)

# Combine normalized data with categorical data (temporarily)
df_normalized[categorical_columns] = df[categorical_columns].reset_index(drop=True)

# Report after normalization
print("Data preview after normalization:\n", df_normalized.head())

from sklearn.preprocessing import OneHotEncoder

# Encode categorical variables
encoder = OneHotEncoder(sparse=False, drop='first')
encoded_categorical = encoder.fit_transform(df[categorical_columns])
encoded_df = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out(categorical_columns))

# Combine encoded categorical data with normalized data
df_final = pd.concat([df_normalized[numeric_columns], encoded_df], axis=1)

# Add the target column back to the final dataframe
df_final[target_column] = output.reset_index(drop=True)

# Check columns in df_final to ensure 'output' is present
print("Columns in df_final:", df_final.columns)

# Report after encoding
print("Data preview after encoding:\n", df_final.head())
print("Final data shape:", df_final.shape)

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import networkx as nx
import pandas as pd
import networkx as nx

# Assuming df is correctly loaded and has 'output' column

plt.figure(figsize=(10, 6))
ax = sns.countplot(x='output', data=df_final)
plt.title('Distribution of Heart Attack Risk')
plt.xlabel('Heart Attack Risk')
plt.ylabel('Population')
plt.xticks([0, 1], ['Lesser Chance of Heartattack', 'More Chance of Heartattack'])

# Annotate axes inside the plot
plt.text(0.02, 0.95, 'X-axis: Heart Attack Risk\nY-axis: Population', ha='left', va='top', transform=ax.transAxes, bbox=dict(facecolor='lightgray', alpha=0.5))

plt.tight_layout()  # Adjust layout to prevent overlap
plt.show()

plt.figure(figsize=(10, 6))
plt.hist(df['age'], bins=range(20, 91, 5), color='skyblue', edgecolor='black')
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')

# Annotate axes inside the plot
plt.text(0.02, 0.95, 'X-axis: Age\nY-axis: Frequency', ha='left', va='top', transform=plt.gca().transAxes, bbox=dict(facecolor='lightgray', alpha=0.5))
plt.show()

# Ensuring age is represented as integers
df['age'] = df['age'].astype(int)

plt.figure(figsize=(10, 6))
plt.scatter(df['age'], df['chol'], alpha=0.5)
plt.title('Age vs Cholesterol')
plt.xlabel('Age')
plt.ylabel('Cholesterol')

# Annotate axes inside the plot
plt.text(0.02, 0.95, 'X-axis: Age\nY-axis: Cholesterol', ha='left', va='top', transform=plt.gca().transAxes, bbox=dict(facecolor='lightgray', alpha=0.5))

plt.show()

plt.figure(figsize=(10, 8))
sns.heatmap(df_final.corr(), annot=True, cmap='coolwarm', linewidths=.5)
plt.title('Correlation Matrix Heatmap')

plt.show()

plt.figure(figsize=(10, 6))
plt.scatter(df['age'], df['thalachh'], alpha=0.5)
plt.title('Age vs Maximum Heart Rate Achieved')
plt.xlabel('Age')
plt.ylabel('Maximum Heart Rate Achieved')

# Annotate axes inside the plot
plt.text(0.02, 0.95, 'X-axis: Age\nY-axis: Max Heart Rate', ha='left', va='top', transform=plt.gca().transAxes, bbox=dict(facecolor='lightgray', alpha=0.5))

plt.show()

plt.figure(figsize=(10, 8))
key_features = ['age', 'trtbps', 'chol', 'thalachh', 'output']
sns.heatmap(df_final[key_features].corr(), annot=True, cmap='coolwarm', linewidths=.5)
plt.title('Correlation Matrix of Key Features')

# Annotate axes inside the plot
plt.text(0.02, 0.95, 'X-axis: Variables\nY-axis: Variables', ha='left', va='top', transform=plt.gca().transAxes, bbox=dict(facecolor='lightgray', alpha=0.5))

plt.show()

# Generate the plots
plt.figure(figsize=(14, 10))

# Plot 1: Stacked bar chart (left)
plt.subplot(1, 2, 1)
pd.crosstab(df['cp'], df['sex']).plot(kind='bar', stacked=True, ax=plt.gca())
plt.title('Stacked Bar Chart of Chest Pain Types by Sex')
plt.xlabel('Chest Pain Type')
plt.ylabel('Count')
plt.xticks(rotation=0)  # Rotate x-axis labels to be horizontal

# Plot 2: Pie chart (right)
plt.subplot(1, 2, 2)
cp_counts = df['cp'].value_counts()
plt.pie(cp_counts, labels=cp_counts.index, autopct='%1.1f%%', colors=sns.color_palette('viridis', len(cp_counts)))
plt.title('Proportion of Chest Pain Types')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.xticks()  # Ensure x-axis ticks are default (horizontal)

plt.tight_layout()  # Adjust layout for better spacing
plt.show()

plt.figure(figsize=(10, 6))
plt.hist(df['chol'], bins=range(0, 451, 10), color='skyblue', edgecolor='black')
plt.title('Distribution of Cholesterol Levels')
plt.xlabel('Cholesterol Level')
plt.ylabel('Frequency')

# Annotate axes inside the plot
plt.text(0.02, 0.95, 'X-axis: Cholesterol Level\nY-axis: Frequency', ha='left', va='top', transform=plt.gca().transAxes, bbox=dict(facecolor='lightgray', alpha=0.5))

plt.show()

import networkx as nx

# Create a graph
G = nx.Graph()

# Add edges based on correlations
edges = [('age', 'thalachh'), ('age', 'chol'), ('thalachh', 'output'), ('chol', 'output')]
G.add_edges_from(edges)

plt.figure(figsize=(8, 6))
nx.draw(G, with_labels=True, node_color='skyblue', node_size=2000, font_size=12, font_color='black', edge_color='gray')
plt.title('Graph Representation of Key Relationships')

# Annotate axes inside the plot
plt.text(0.02, 0.95, 'X-axis: Nodes\nY-axis: Nodes', ha='left', va='top', transform=plt.gca().transAxes, bbox=dict(facecolor='lightgray', alpha=0.5))

plt.show()

# List of columns to analyze
to_analyze = ['age', 'trtbps', 'chol', 'thalachh']

# Generate box plots for each column if it exists in df
for column in to_analyze:
    if column in df.columns:
        df.boxplot(column=column, figsize=(8, 6))
        plt.title(f'Box Plot of {column}')
        plt.ylabel('Value')
        plt.show()
    else:
        print(f"Column {column} is not found in the DataFrame.")

# Load your dataset into df
df = pd.read_csv('/content/Heartattack.csv')

# Check if 'chol' and 'output' columns exist in the DataFrame
if 'chol' in df.columns and 'output' in df.columns:
    # Separate data for higher and lower chance of heart attack
    higher_chance = df[df['output'] == 1]['chol']
    lower_chance = df[df['output'] == 0]['chol']

    # Create a figure with a single subplot
    plt.figure(figsize=(12, 6))

    # Plotting the histogram for higher chance of heart attack in red
    sns.histplot(higher_chance, bins=range(100, 451, 10), stat='count', kde=True, color='red', label='Higher Chance of Heart Attack')

    # Plotting the histogram for lower chance of heart attack in blue
    sns.histplot(lower_chance, bins=range(100, 451, 10), stat='count', kde=True, color='blue', label='Lower Chance of Heart Attack')

    # Set labels and title
    plt.title('Distribution of Cholesterol Level for Different Chances of Heart Attack', fontsize=15)
    plt.xlabel('Cholesterol Level', fontsize=12)
    plt.ylabel('Count', fontsize=12)  # Change y-axis label to Count
    plt.legend()  # Show legend with labels

    # Display the plot
    plt.show()

else:
    print("Columns 'chol' or 'output' not found in the DataFrame.")

# Check if 'trtbps' and 'output' columns exist in the DataFrame
if 'trtbps' in df.columns and 'output' in df.columns:
    # Separate data for higher and lower chance of heart attack
    higher_chance = df[df['output'] == 1]['trtbps']
    lower_chance = df[df['output'] == 0]['trtbps']

    # Create a figure
    plt.figure(figsize=(10, 6))

    # Plotting the histogram for higher chance of heart attack in red
    sns.histplot(higher_chance, bins=range(75, 206, 5), kde=True, color='red', linewidth=2, label='Higher Chance of Heart Attack', stat='count')

    # Adding KDE plot for higher chance of heart attack
    sns.histplot(higher_chance, bins=range(75, 206, 5), kde=True, color='red', linewidth=2, label='KDE: Higher Chance', stat='count')

    # Plotting the histogram for lower chance of heart attack in blue
    sns.histplot(lower_chance, bins=range(75, 206, 5), kde=False, color='blue', label='Lower Chance of Heart Attack', stat='count')

    # Adding KDE plot for lower chance of heart attack
    sns.histplot(lower_chance, bins=range(75, 206, 5), kde=True, color='blue', linewidth=2, label='KDE: Lower Chance', stat='count')

    # Set labels and title
    plt.title('Distribution of Blood Pressure for Different Chances of Heart Attack', fontsize=15)
    plt.xlabel('Blood Pressure', fontsize=12)
    plt.ylabel('Count', fontsize=12)  # Set y-axis as Count
    plt.legend()  # Show legend with labels

    # Display the plot
    plt.show()

else:
    print("Columns 'trtbps' or 'output' not found in the DataFrame.")

# Check if 'thalachh' and 'output' columns exist in the DataFrame
if 'thalachh' in df.columns and 'output' in df.columns:
    # Separate data for higher and lower chance of heart attack
    higher_chance = df[df['output'] == 1]['thalachh']
    lower_chance = df[df['output'] == 0]['thalachh']

    # Create a figure
    plt.figure(figsize=(10, 6))

    # Plotting the histograms with counts and overlaying KDE plot for higher chance
    sns.histplot(higher_chance, bins=range(65, 211, 5), kde=True, color='red', label='Higher Chance of Heart Attack', stat='count')

    # Plotting the histograms with counts and overlaying KDE plot for lower chance
    sns.histplot(lower_chance, bins=range(65, 211, 5), kde=True, color='blue', label='Lower Chance of Heart Attack', stat='count')

    # Set labels and title
    plt.title('Distribution of Heart Rate for Different Chances of Heart Attack', fontsize=15)
    plt.xlabel('Heart Rate', fontsize=12)
    plt.ylabel('Count', fontsize=12)  # Set y-axis as Count
    plt.legend()  # Show legend with labels

    # Display the plot
    plt.show()

else:
    print("Columns 'thalachh' or 'output' not found in the DataFrame.")

plt.figure(figsize=(12, 10))
sns.pairplot(df, vars=['age', 'trtbps', 'chol', 'thalachh'], hue='output')
plt.suptitle('Pair Plot of Key Variables', y=1.02)
plt.show()

# Importing necessary libraries
import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('/content/Heartattack.csv')  # Update the path accordingly

# Selecting features for clustering
features = ['age', 'trtbps', 'chol']
X = df[features]

# Applying K-Means Clustering
kmeans = KMeans(n_clusters=4, random_state=42)
df['kmeans_cluster'] = kmeans.fit_predict(X)


# Plotting the clusters
plt.figure(figsize=(12, 8))
sns.scatterplot(x='age', y='chol', hue='kmeans_cluster', palette='viridis', data=df, s=100)
plt.title('K-Means Clustering of Heart Attack Data')
plt.xlabel('Age')
plt.ylabel('Cholesterol Level')
plt.legend(title='Cluster')
plt.show()

# Plotting Blood Pressure vs Cholesterol with clusters
plt.figure(figsize=(12, 8))
sns.scatterplot(x='trtbps', y='chol', hue='kmeans_cluster', palette='viridis', data=df, s=100)
plt.title('K-Means Clustering of Heart Attack Data')
plt.xlabel('Resting Blood Pressure')
plt.ylabel('Cholesterol Level')
plt.legend(title='Cluster')
plt.show()

import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Load the dataset
df = pd.read_csv('/content/Heartattack.csv')  # Update the path accordingly

def perform_kmeans_clustering(features, full_forms, n_clusters=3):
    X = df[features]

    # Applying K-Means Clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    df['kmeans_cluster'] = kmeans.fit_predict(X)

    # Sorting the DataFrame by cluster labels
    sorted_df = df.sort_values(by='kmeans_cluster')

    # Print the sorted DataFrame
    print(f"\nSorted DataFrame for features: {features}")
    print(sorted_df)

    # Calculate summary statistics for each cluster
    cluster_summary = sorted_df.groupby('kmeans_cluster')[features].agg(['mean', 'std'])
    print(f"\nCluster Summary for features: {features}")
    print(cluster_summary)

    # Additional descriptive statistics
    cluster_description = sorted_df.groupby('kmeans_cluster')[features].describe()
    print(f"\nCluster Description for features: {features}")
    print(cluster_description)

    # Plotting K-Means Clusters in 3D
    fig = plt.figure(figsize=(15, 15))
    ax = fig.add_subplot(111, projection='3d')
    scatter = ax.scatter(sorted_df[features[0]], sorted_df[features[1]], sorted_df[features[2]], c=sorted_df['kmeans_cluster'], cmap='viridis', s=100)

    # Adding labels and title
    ax.set_title(f'K-Means Clustering of HeartAttack Data for features: {", ".join(full_forms)}')
    ax.set_xlabel(full_forms[0])
    ax.set_ylabel(full_forms[1])
    ax.set_zlabel(full_forms[2])
    legend = ax.legend(*scatter.legend_elements(), title='Cluster')
    ax.add_artist(legend)

    # Show the plot
    plt.show()

# 1st case: Age, Cholesterol, Fasting Blood Sugar (fbs)
perform_kmeans_clustering(
    ['age', 'chol', 'fbs'],
    ['Age', 'Cholesterol', 'Fasting Blood Sugar']
)

# 2nd case: Age, Cholesterol, Resting Blood Pressure (trtbps)
perform_kmeans_clustering(
    ['age', 'chol', 'trtbps'],
    ['Age', 'Cholesterol', 'Resting Blood Pressure']
)

# 3rd case: Age, Cholesterol, Maximum Heart Rate Achieved (thalachh)
perform_kmeans_clustering(
    ['age', 'chol', 'thalachh'],
    ['Age', 'Cholesterol', 'Maximum Heart Rate Achieved']
)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report
import matplotlib.pyplot as plt


# Load the dataset
df = pd.read_csv('/content/Heartattack.csv')

# Handling missing values (if any)
df.fillna(df.mean(), inplace=True)

# Split the data into features and target variable
X = df.drop('output', axis=1)
y = df['output']

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=65)

# Logistic Regression
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
y_pred_log_reg = log_reg.predict(X_test)


# Evaluation
print("Logistic Regression:")
print(f'Accuracy: {accuracy_score(y_test, y_pred_log_reg)}')
print(f'Precision: {precision_score(y_test, y_pred_log_reg)}')
print(f'Recall: {recall_score(y_test, y_pred_log_reg)}')
print(f'F1 Score: {f1_score(y_test, y_pred_log_reg)}')
print(f'ROC-AUC: {roc_auc_score(y_test, y_pred_log_reg)}')
print(classification_report(y_test, y_pred_log_reg))

from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import pandas as pd

# Load the dataset
df = pd.read_csv('/content/Heartattack.csv')

# Handling missing values (if any)
df.fillna(df.mean(), inplace=True)

# Split the data into features and target variable
X = df.drop('output', axis=1)
y = df['output']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=65)

# Train the Decision Tree model with regularization
tree_clf = DecisionTreeClassifier(max_depth=5, min_samples_split=10, min_samples_leaf=5, random_state=65)
tree_clf.fit(X_train, y_train)

# Evaluate with cross-validation
cv_scores = cross_val_score(tree_clf, X, y, cv=5)
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Accuracy:", cv_scores.mean())

# Evaluate on the training set
train_accuracy = accuracy_score(y, tree_clf.predict(X))
print("Training Accuracy:", train_accuracy)

# Predict the outcomes on the test set
y_pred_tree = tree_clf.predict(X_test)

# Evaluate the model
print("Decision Tree Model Performance:")
print(f'Accuracy: {accuracy_score(y_test, y_pred_tree)}')
print(classification_report(y_test, y_pred_tree))

# Visualize the Decision Tree
plt.figure(figsize=(20,10))
plot_tree(tree_clf, filled=True, feature_names=X.columns, class_names=['0', '1'], rounded=True)
plt.show()

# Feature importance
importances = tree_clf.feature_importances_
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)
print("Feature Importance:")
print(feature_importance_df)

# Example prediction for a new data point
new_data_point = pd.DataFrame([[38, 1, 2, 138, 175, 0, 1, 173, 0, 0, 2, 4, 2]],
                              columns=X.columns)

prediction = tree_clf.predict(new_data_point)
prediction_probability = tree_clf.predict_proba(new_data_point)

print(f"Prediction: {prediction}")
print(f"Probability: {prediction_probability}")

# Step 1: Import Libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Step 2: Load and Prepare the Dataset
# Assuming df is already loaded and preprocessed
# Features and target variable
X = df[[ 'age', 'trtbps', 'chol', 'thalachh','cp','fbs'	,'restecg',		'oldpeak',	'slp',	'caa',	'thall']]
y = df['output']

# Step 3: Split the Data into Training and Testing Sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Train and Evaluate Linear Regression Model
linear_regressor = LinearRegression()
linear_regressor.fit(X_train, y_train)
y_pred_lr = linear_regressor.predict(X_test)

# Evaluate Linear Regression Model
mse_lr = mean_squared_error(y_test, y_pred_lr)
r2_lr = r2_score(y_test, y_pred_lr)
print(f'Linear Regression - MSE: {mse_lr}, R2: {r2_lr}')

# Step 5: Train and Evaluate Decision Tree Regressor
tree_regressor = DecisionTreeRegressor(random_state=42)
tree_regressor.fit(X_train, y_train)
y_pred_tree = tree_regressor.predict(X_test)

# Evaluate Decision Tree Regressor
mse_tree = mean_squared_error(y_test, y_pred_tree)
r2_tree = r2_score(y_test, y_pred_tree)
print(f'Decision Tree Regressor - MSE: {mse_tree}, R2: {r2_tree}')

# Plotting Actual vs Predicted Values
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_test, y_pred_lr, color='blue')
plt.plot([y.min(), y.max()], [y.min(), y.max()], color='red', linewidth=2)
plt.title('Linear Regression: Actual vs Predicted')
plt.xlabel('Actual')
plt.ylabel('Predicted')

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_pred_tree, color='green')
plt.plot([y.min(), y.max()], [y.min(), y.max()], color='red', linewidth=2)
plt.title('Decision Tree Regressor: Actual vs Predicted')
plt.xlabel('Actual')
plt.ylabel('Predicted')

plt.tight_layout()
plt.show()

import pandas as pd

# Load your dataset
dataset_path = '/content/Heartattack.csv'  # Update this with the actual path to your dataset
df = pd.read_csv(dataset_path)

# Handle missing values (if any)
df.fillna(df.mean(), inplace=True)

# Provided results
initial_shape = df.shape
missing_values = df.isnull().sum()
data_shape_after_handling = df.shape
memory_usage_kb = df.memory_usage(deep=True).sum() / 1024  # Memory usage in KB
memory_usage_mb = memory_usage_kb / 1024  # Convert KB to MB

# Dataset information
data_info = {
    'entries': len(df),
    'features': df.shape[1],
    'float_cols': df.select_dtypes(include=['float64']).shape[1],
    'int_cols': df.select_dtypes(include=['int64']).shape[1],
}

# Generate GPT-like insights
insights = f"""
### Key Insights from the Heart Attack Dataset Analysis ###

1. **Initial Data Shape**:
   - The dataset initially comprises {initial_shape[0]} entries and {initial_shape[1]} columns.

2. **Data Completeness**:
   - There are no missing values in the dataset. Each column is fully populated, ensuring that no data points are lost or omitted. This completeness is crucial for accurate model training and analysis.

3. **Dataset Structure**:
   - The dataset includes {data_info['int_cols']} integer columns and {data_info['float_cols']} float column. The integer columns predominantly represent categorical data encoded as integers, while the single float column captures continuous data.

4. **Memory Usage**:
   - The dataset's memory footprint is relatively small, utilizing approximately {memory_usage_mb:.2f} MB of memory. This makes it efficient to handle and process, even on machines with limited resources.

5. **Feature Overview**:
   - The dataset covers a variety of features related to heart health and demographic information. This includes medical measurements (e.g., cholesterol levels, resting blood pressure) and patient characteristics (e.g., age, sex). Such diversity in features is essential for building robust predictive models.

6. **Target Variable**:
   - The target variable 'output' is present in the dataset and is crucial for any predictive modeling tasks aimed at identifying the likelihood of a heart attack.

### Recommendations for Further Analysis ###

1. **Exploratory Data Analysis (EDA)**:
   - Conduct comprehensive EDA to visualize data distributions and relationships between features. Use plots such as histograms, box plots, and scatter plots to identify patterns, trends, and potential outliers.

2. **Correlation Analysis**:
   - Perform correlation analysis to understand the relationships between features and the target variable. Identifying highly correlated features can help in feature selection and engineering.

3. **Feature Engineering**:
   - Consider creating new features or transforming existing ones to enhance predictive power. This could include combining features, extracting relevant components, or normalizing data.

4. **Model Training and Evaluation**:
   - Train various machine learning models (e.g., logistic regression, decision trees) using the dataset. Evaluate model performance using metrics such as accuracy, precision, recall, F1 score, and ROC-AUC to identify the best-performing model.

5. **Cross-Validation**:
   - Implement cross-validation techniques to ensure that the model generalizes well to unseen data. This helps in avoiding overfitting and provides a more robust estimate of model performance.

By following these steps, we can gain deeper insights into the dataset and build effective models for predicting heart attack risk.
"""

print(insights)

import pandas as pd

# Load your dataset
dataset_path = '/content/Heartattack.csv'  # Update this with the actual path to your dataset
df = pd.read_csv(dataset_path)

# Provided outlier detection results
outliers = {
    'age': 0,
    'sex': 0,
    'cp': 0,
    'trtbps': 2,
    'chol': 4,
    'fbs': 0,
    'restecg': 0,
    'thalachh': 1,
    'exng': 0,
    'oldpeak': 2,
    'slp': 0,
    'caa': 5,
    'thall': 2,
    'output': 0
}

data_shape_after_removal = (287, 14)

# Generate GPT-like insights on outlier detection
insights = f"""
### Insights from Outlier Detection ###

1. **Outlier Analysis**:
   - Outliers were detected in several features:
     - `trtbps`: {outliers['trtbps']} outliers
     - `chol`: {outliers['chol']} outliers
     - `thalachh`: {outliers['thalachh']} outliers
     - `oldpeak`: {outliers['oldpeak']} outliers
     - `caa`: {outliers['caa']} outliers
     - `thall`: {outliers['thall']} outliers
   - These outliers represent extreme values that deviate significantly from the rest of the data. Identifying and addressing these outliers is crucial for improving the quality and accuracy of any subsequent analysis or predictive modeling.

2. **Impact on Data Shape**:
   - After removing the detected outliers, the dataset's shape was reduced from 303 entries to {data_shape_after_removal[0]} entries.
   - This reduction in dataset size reflects the removal of data points that could potentially distort analysis and model performance.

3. **Feature-Specific Insights**:
   - `trtbps` (Resting Blood Pressure) and `chol` (Cholesterol Level) had relatively few outliers ({outliers['trtbps']} and {outliers['chol']} respectively). This indicates that most patients have blood pressure and cholesterol levels within a normal range.
   - `caa` (Number of Major Vessels Colored by Fluoroscopy) had the highest number of outliers ({outliers['caa']}), suggesting that there are a few patients with an unusual number of major vessels colored, which might be significant in understanding heart disease progression.
   - `oldpeak` (ST Depression Induced by Exercise) and `thall` (Thalassemia) had {outliers['oldpeak']} and {outliers['thall']} outliers each, indicating that these measures also have a few extreme values that might affect the overall analysis.

4. **Data Integrity**:
   - Removing outliers ensures that the dataset is cleaner and more representative of the typical patient profile. This step is essential for any reliable statistical analysis or machine learning model training.
"""

print(insights)

# Calculate the count of each class in the 'output' column
output_counts = df['output'].value_counts()

# Display the results
print("Distribution of Heart Attack Risk:")
print(f"Lesser Chance of Heart Attack (0): {output_counts[0]} patients")
print(f"More Chance of Heart Attack (1): {output_counts[1]} patients")

import numpy as np

# Calculate frequency counts and bin edges
counts, bins = np.histogram(df['age'], bins=range(20, 91, 5))

# Identify the bin with the maximum frequency
max_freq_bin = bins[np.argmax(counts)]
max_freq_count = np.max(counts)

# Print results
print(f"Age Distribution Analysis:")
print(f"- The most frequent age bin is {max_freq_bin}-{max_freq_bin + 5} years with {max_freq_count} occurrences.")
print(f"- The histogram shows the distribution of ages across various bins, providing insights into the age demographics of the dataset.")

# Calculate correlation between Age and Cholesterol
correlation = df['age'].corr(df['chol'])

# Identify the oldest and youngest individuals
oldest_person = df.loc[df['age'].idxmax()]
youngest_person = df.loc[df['age'].idxmin()]

# Identify the person with highest and lowest cholesterol
highest_chol_person = df.loc[df['chol'].idxmax()]
lowest_chol_person = df.loc[df['chol'].idxmin()]

# Print results
print(f"Age vs Cholesterol Analysis:")
print(f"- There is a correlation of {correlation:.2f} between Age and Cholesterol.")
print(f"- Oldest person: Age {oldest_person['age']}, Cholesterol {oldest_person['chol']}")
print(f"- Youngest person: Age {youngest_person['age']}, Cholesterol {youngest_person['chol']}")
print(f"- Person with highest cholesterol: Age {highest_chol_person['age']}, Cholesterol {highest_chol_person['chol']}")
print(f"- Person with lowest cholesterol: Age {lowest_chol_person['age']}, Cholesterol {lowest_chol_person['chol']}")